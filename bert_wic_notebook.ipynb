{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236294aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "import time\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7e6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#citing kathy mckeown for the code in this cell\n",
    "LABELS = ['F', 'T']\n",
    "\n",
    "def get_wic_subset(data_dir):\n",
    "\twic = []\n",
    "\tsplit = data_dir.strip().split('/')[-1]\n",
    "\twith open(os.path.join(data_dir, '%s.data.txt' % split), 'r', encoding='utf-8') as datafile, \\\n",
    "\t\topen(os.path.join(data_dir, '%s.gold.txt' % split), 'r', encoding='utf-8') as labelfile:\n",
    "\t\tfor (data_line, label_line) in zip(datafile.readlines(), labelfile.readlines()):\n",
    "\t\t\tword, _, word_indices, sentence1, sentence2 = data_line.strip().split('\\t')\n",
    "\t\t\tsentence1_word_index, sentence2_word_index = word_indices.split('-')\n",
    "\t\t\tlabel = LABELS.index(label_line.strip())\n",
    "\t\t\twic.append({\n",
    "\t\t\t\t'word': word,\n",
    "\t\t\t\t'sentence1_word_index': int(sentence1_word_index),\n",
    "\t\t\t\t'sentence2_word_index': int(sentence2_word_index),\n",
    "\t\t\t\t'sentence1_words': sentence1.split(' '),\n",
    "\t\t\t\t'sentence2_words': sentence2.split(' '),\n",
    "\t\t\t\t'label': label\n",
    "\t\t\t})\n",
    "\treturn wic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391da7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict_train = get_wic_subset('wic/train')\n",
    "word_dict_test = get_wic_subset('wic/dev')\n",
    "out_file_directory = ('output')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c982dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sent_concat_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(word_dict_train)):\n",
    "    sent_concat_train.append(\" \".join(word_dict_train[i]['sentence1_words']))\n",
    "    sent_concat_train.append(\" \".join(word_dict_train[i]['sentence2_words']))\n",
    "\n",
    "    y_train.append(word_dict_train[i]['label'])\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\", output_hidden_states = True)\n",
    "encoded_input = tokenizer(sent_concat_train, padding=True, truncation=True,return_tensors='pt') # padding=\"max_length\", truncation=True,\n",
    "\n",
    "\n",
    "N = len(sent_concat_train)\n",
    "i = 0\n",
    "inc_num = 5\n",
    "ii = []\n",
    "tti = []\n",
    "am = []\n",
    "hidden_states = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d5bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16028\\AppData\\Local\\Temp/ipykernel_23704/2693629446.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output=model(input_ids=ii, attention_mask=torch.tensor(am), token_type_ids=tti)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "2969.1\n"
     ]
    }
   ],
   "source": [
    "#We are effectively using a batch size of 5. to speed up the hidden state extraction\n",
    "start = time.time()\n",
    "with torch.no_grad(): #need torch.no_grad() to speed up the process by not keeping the computational graph\n",
    "    while i < len(sent_concat_train):\n",
    "        if i%1000 == 0: print(i)\n",
    "        ii = encoded_input['input_ids'][i:i+inc_num]\n",
    "        tti = encoded_input['token_type_ids'][i:i+inc_num]\n",
    "        am = encoded_input['attention_mask'][i:i+inc_num]\n",
    "\n",
    "        output=model(input_ids=ii, attention_mask=torch.tensor(am), token_type_ids=tti)\n",
    "        hidden_states.append(output.last_hidden_state)\n",
    "        i = i + inc_num \n",
    "        # this is catching 0:5->5:10->10:15 ->45:50->50:55 (i think it's working?')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(round(end-start,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92da7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_list = []\n",
    "for tens in hidden_states:\n",
    "    for i in tens:\n",
    "        srmw = torch.mean(i, dim=0) \n",
    "        total_list.append(srmw)\n",
    "\n",
    "sent1_embeds_train = []\n",
    "sent2_embeds_train = []\n",
    "\n",
    "for i in range(len(total_list)):\n",
    "    if i%2 == 0: \n",
    "        sent1_embeds_train.append(total_list[i])\n",
    "    else:\n",
    "        sent2_embeds_train.append(total_list[i])\n",
    "\n",
    "both_list_train = []\n",
    "for i in range(len(sent1_embeds_train)):\n",
    "    both_list_train.append([sent1_embeds_train[i], sent2_embeds_train[i]])\n",
    "\n",
    "\n",
    "diffs_train = []\n",
    "cosine_sims_train = []\n",
    "euclidean_dist_train = []\n",
    "l1_dist_train = []\n",
    "for i in range(len(sent1_embeds_train)):\n",
    "    sent1_vec = both_list_train[i][0].reshape(1,-1)\n",
    "    sent2_vec = both_list_train[i][1].reshape(1,-1)\n",
    "    cosine_sims_train.append(cosine_similarity(sent1_vec, sent2_vec)[0][0])\n",
    "    dif = sent1_vec - sent2_vec\n",
    "    diffs_train.append(torch.squeeze(dif, dim=0))\n",
    "\n",
    "    euclidean_dist_train.append(np.linalg.norm(sent1_vec - sent2_vec))\n",
    "    l1_dist_train.append(np.linalg.norm(sent1_vec - sent2_vec, ord=1))  \n",
    "\n",
    "for i in range(len(diffs_train)):\n",
    "    diffs_train[i] = np.array(diffs_train[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695ad9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16028\\AppData\\Local\\Temp/ipykernel_23704/3148722437.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output=model(input_ids=ii, attention_mask=torch.tensor(am), token_type_ids=tti)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "168.0\n"
     ]
    }
   ],
   "source": [
    "sent_concat_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(len(word_dict_test)):\n",
    "    sent_concat_test.append(\" \".join(word_dict_test[i]['sentence1_words']))\n",
    "    sent_concat_test.append(\" \".join(word_dict_test[i]['sentence2_words']))\n",
    "\n",
    "    y_test.append(word_dict_test[i]['label'])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\", output_hidden_states = True)\n",
    "encoded_input = tokenizer(sent_concat_test, padding=True, truncation=True,return_tensors='pt') # padding=\"max_length\", truncation=True,\n",
    "\n",
    "start = time.time()\n",
    "N = len(sent_concat_test)\n",
    "i = 0\n",
    "inc_num = 5\n",
    "ii = []\n",
    "tti = []\n",
    "am = []\n",
    "hidden_states_test = []\n",
    "#repeating code used on the train subset\n",
    "with torch.no_grad(): \n",
    "    while i < len(sent_concat_test):\n",
    "        if i%100 == 0: print(i)\n",
    "        ii = encoded_input['input_ids'][i:i+inc_num]\n",
    "        tti = encoded_input['token_type_ids'][i:i+inc_num]\n",
    "        am = encoded_input['attention_mask'][i:i+inc_num]\n",
    "\n",
    "        output=model(input_ids=ii, attention_mask=torch.tensor(am), token_type_ids=tti)\n",
    "        hidden_states_test.append(output.last_hidden_state)\n",
    "        i = i + inc_num \n",
    "end = time.time()\n",
    "print(round(end-start,1))\n",
    "\n",
    "\n",
    "total_list = []\n",
    "for tens in hidden_states_test:\n",
    "    for i in tens:\n",
    "        srmw = torch.mean(i, dim=0) \n",
    "        total_list.append(srmw)\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "\n",
    "sent1_embeds_test = []\n",
    "sent2_embeds_test = []\n",
    "\n",
    "for i in range(len(total_list)):\n",
    "    if i%2 == 0: \n",
    "        sent1_embeds_test.append(total_list[i])\n",
    "    else:\n",
    "        sent2_embeds_test.append(total_list[i])\n",
    "\n",
    "both_list_test = []\n",
    "for i in range(len(sent1_embeds_test)):\n",
    "    both_list_test.append([sent1_embeds_test[i], sent2_embeds_test[i]])\n",
    "\n",
    "\n",
    "diffs_test = []\n",
    "cosine_sims_test = []\n",
    "euclidean_dist_test = []\n",
    "l1_dist_test = []\n",
    "for i in range(len(sent1_embeds_test)):\n",
    "    sent1_vec = both_list_test[i][0].reshape(1,-1)\n",
    "    sent2_vec = both_list_test[i][1].reshape(1,-1)\n",
    "    cosine_sims_test.append(cosine_similarity(sent1_vec, sent2_vec)[0][0])\n",
    "    dif = sent1_vec - sent2_vec\n",
    "    diffs_test.append(torch.squeeze(dif, dim=0))\n",
    "\n",
    "    euclidean_dist_test.append(np.linalg.norm(sent1_vec - sent2_vec))\n",
    "    l1_dist_test.append(np.linalg.norm(sent1_vec - sent2_vec, ord=1))\n",
    "\n",
    "\n",
    "for i in range(len(diffs_test)):\n",
    "    diffs_test[i] = np.array(diffs_test[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f957c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left in the diffs features above for testing later but after experimenting,\n",
      "ultimately decided on just 3 features: cosine similiarity, eucliden distance, and l1_distance\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.stack(diffs_train).shape\n",
    "X_train_part1 = np.stack(diffs_train)\n",
    "X_train_part2 = np.vstack([cosine_sims_train, euclidean_dist_train, l1_dist_train] ).transpose()\n",
    "# X_train = np.hstack([X_train_part1, X_train_part2])\n",
    "\n",
    "np.stack(diffs_test).shape\n",
    "X_test_part1 = np.stack(diffs_test)\n",
    "X_test_part2 = np.vstack([cosine_sims_test, euclidean_dist_test, l1_dist_test] ).transpose()\n",
    "# X_test = np.hstack([X_test_part1, X_test_part2]) #after testing landed on \n",
    "\n",
    "X_train = X_train_part2 #better to ignore the embeddings that lead to overfitting\n",
    "X_test = X_test_part2 #better to ignoret the embeddings that lead to overfitting\n",
    "\n",
    "mystr = \"\"\"Left in the diffs features above for testing later but after experimenting,\n",
    "ultimately decided on just 3 features: cosine similiarity, eucliden distance, and l1_distance\"\"\"\n",
    "print(mystr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01dc8cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6330140014738393\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, max_iter=2000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_preds = clf.predict(X_train)\n",
    "print('Training accuracy: ' + str(accuracy_score(y_train, y_train_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89706f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 1 0 1 0]\n",
      "Finally, the testing accuracy: 0.6363636363636364\n",
      "Not bad with a simple logistic regression model!\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, max_iter=2000)\n",
    "clf.fit(X_train, y_train) #********************** again need to fit on training\n",
    "y_test_preds = clf.predict(X_test)\n",
    "\n",
    "#Saving to an output file in case it's useful later\n",
    "y_outfile_format = []\n",
    "for i in range(len(y_test_preds)):\n",
    "    if y_test_preds[i] == 1:\n",
    "        y_outfile_format.append('T')\n",
    "    elif y_test_preds[i] == 0:\n",
    "        y_outfile_format.append('F')\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "with open('bert_wic_outfile.txt', 'w') as f:\n",
    "    for item in y_outfile_format:\n",
    "        f.write(item)\n",
    "        f.write('\\n')\n",
    "print(y_test_preds[:10])\n",
    "print('Finally, the testing accuracy: ' + str(accuracy_score(y_test, y_test_preds)))\n",
    "print('Not bad with a simple logistic regression model!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
